{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1054465,"sourceType":"datasetVersion","datasetId":541298}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n!pip install ta\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport ta\nimport math\nimport warnings\nimport tensorflow as tf\n\nwarnings.filterwarnings('ignore')\n\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom torch.utils.data import DataLoader\nfrom copy import deepcopy as dc\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, r2_score\nfrom pathlib import Path\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-28T04:55:50.650575Z","iopub.execute_input":"2023-11-28T04:55:50.650935Z","iopub.status.idle":"2023-11-28T04:56:23.106334Z","shell.execute_reply.started":"2023-11-28T04:55:50.650887Z","shell.execute_reply":"2023-11-28T04:56:23.105148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:56:23.108307Z","iopub.execute_input":"2023-11-28T04:56:23.108947Z","iopub.status.idle":"2023-11-28T04:56:23.116937Z","shell.execute_reply.started":"2023-11-28T04:56:23.108902Z","shell.execute_reply":"2023-11-28T04:56:23.115854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We can recommend by certain criteria\n\nRisk Profile: Assess the risk associated with the stock/ETF, considering factors like volatility, beta, standard deviation, and historical performance. Investors have different risk tolerances, so offering a range of risk profiles can cater to varying preferences.\n\nPerformance Metrics: Analyze historical returns, Sharpe ratio, alpha, beta, and other performance metrics to gauge the past performance and potential future returns.\n\nIndustry/ Sector: Segment stocks/ETFs based on industries or sectors they belong to. Some investors may prefer exposure to specific sectors (tech, healthcare, energy, etc.) due to their understanding or belief in their growth potential.\n\nDiversification: Provide recommendations that aid in diversifying a portfolio across different industries, regions, or asset classes. Diversification helps mitigate risk.\n\nMarket Trends and Sentiment Analysis: Incorporate sentiment analysis by tracking news sentiment, social media chatter, and expert opinions to gauge market trends and sentiment toward particular stocks/ETFs.\n\nFundamental Analysis: Consider fundamental factors such as earnings, revenue growth, profit margins, debt levels, P/E ratios, and other financial metrics to assess the underlying strength of the company.\n\nTechnical Analysis: For traders interested in technical indicators, consider metrics like moving averages, RSI (Relative Strength Index), MACD (Moving Average Convergence Divergence), etc., to evaluate entry and exit points.\n\nLiquidity and Volume: Include stocks/ETFs with sufficient liquidity and trading volume, as low liquidity can impact trade execution and pricing.\n\nCosts and Fees: Consider expense ratios, trading fees, and other costs associated with investing in the recommended stocks/ETFs.\n\nUser Preferences and Constraints: Take into account individual user preferences, investment goals, time horizon, tax considerations, and any constraints or ethical considerations they might have.\n\nRegulatory and Legal Factors: Ensure compliance with regulatory requirements and ethical standards related to investment recommendations.\n\nAdaptability and Learning: Implement a system that continuously learns from user feedback and adapts recommendations based on the changing market conditions and user preferences.\n\nPerformance Against Benchmarks: Compare the recommended stocks/ETFs against relevant benchmarks (like S&P 500, sector-specific indices) to provide a benchmark-relative assessment.","metadata":{}},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"# Write the CSV file\nsymbols_metadata_df = pd.read_csv('/kaggle/input/stock-market-dataset/symbols_valid_meta.csv')\n\nurl = \"https://en.wikipedia.org/wiki/Nasdaq-100\"\nnasdaq100_meta = pd.read_html(url)[4]\n\netf_csv_files = glob.glob(\"/kaggle/input/stock-market-dataset/etfs/*.csv\")\n\nstock_csv_files = glob.glob(\"/kaggle/input/stock-market-dataset/stocks/*.csv\")\n\netf_dfs = {}\nstock_dfs = {}\n\nfor etf_csv_filename in etf_csv_files:\n    df = pd.read_csv(etf_csv_filename, index_col=None, header=0)\n    etf_dfs[Path(etf_csv_filename).stem] = df\n    \nfor stock_csv_filename in stock_csv_files:\n    df = pd.read_csv(stock_csv_filename, index_col=None, header=0)\n    stock_dfs[Path(stock_csv_filename).stem] = df\n    \nstock_dfs['A'] # Example of how you would access a dataframe for a particular ticker\nprint(\"Count of PDs for each ticker: \" + str(len(stock_dfs))) ","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:56:23.118744Z","iopub.execute_input":"2023-11-28T04:56:23.119144Z","iopub.status.idle":"2023-11-28T04:58:59.259909Z","shell.execute_reply.started":"2023-11-28T04:56:23.119115Z","shell.execute_reply":"2023-11-28T04:58:59.258754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_dfs['A'] = pd.read_csv(\"/kaggle/input/stock-market-dataset/stocks/A.csv\", index_col=None, header=0)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:59:02.461987Z","iopub.execute_input":"2023-11-28T04:59:02.462410Z","iopub.status.idle":"2023-11-28T04:59:02.480517Z","shell.execute_reply.started":"2023-11-28T04:59:02.462379Z","shell.execute_reply":"2023-11-28T04:59:02.479254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_dfs['A'].head()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:59:02.830165Z","iopub.execute_input":"2023-11-28T04:59:02.830581Z","iopub.status.idle":"2023-11-28T04:59:02.856217Z","shell.execute_reply.started":"2023-11-28T04:59:02.830547Z","shell.execute_reply":"2023-11-28T04:59:02.854983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define helper functions for analyzing data","metadata":{}},{"cell_type":"code","source":"def plot_histograms( df , variables , n_rows , n_cols ):\n    fig = plt.figure( figsize = ( 16 , 12 ) )\n    for i, var_name in enumerate( variables ):\n        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n        hist, edges = np.histogram(df[var_name], bins=10)\n        df[ var_name ].hist( bins=10 , ax=ax )\n        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n        ax.set_xticklabels( edges , visible=True )\n        ax.set_yticklabels( [] , visible=True )\n    fig.tight_layout()  # Improves appearance a bit.\n    plt.show()\n\ndef plot_distribution( df , var , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n    facet.map( sns.kdeplot , var , shade= True )\n    facet.set( xlim=( 0 , df[ var ].max() ) )\n    facet.add_legend()\n\ndef plot_categories( df , cat , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , row = row , col = col )\n    facet.map( sns.barplot , cat , target )\n    facet.add_legend()\n    \ndef plot_correlation_map( df ):\n    df = df.select_dtypes(['number'])\n    corr = df.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : .9 }, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 12 }\n    )\n    \ndef describe_more( df ):\n    var = [] ; l = [] ; t = []\n    for x in df:\n        var.append( x )\n        l.append( len( pd.value_counts( df[ x ] ) ) )\n        t.append( df[ x ].dtypes )\n    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n    levels.sort_values( by = 'Levels' , inplace = True )\n    return levels\n\ndef plot_variable_importance( X , y ):\n    tree = DecisionTreeClassifier( random_state = 99 )\n    tree.fit( X , y )\n    plot_model_var_imp( tree , X , y )\n    \ndef plot_model_var_imp( model , X , y ):\n    imp = pd.DataFrame( \n        model.feature_importances_  , \n        columns = [ 'Importance' ] , \n        index = X.columns \n    )\n    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n    imp[ : 10 ].plot( kind = 'barh' )\n    print (model.score( X , y ))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:59:05.631787Z","iopub.execute_input":"2023-11-28T04:59:05.632323Z","iopub.status.idle":"2023-11-28T04:59:05.654097Z","shell.execute_reply.started":"2023-11-28T04:59:05.632279Z","shell.execute_reply":"2023-11-28T04:59:05.652954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"code","source":"column_names = symbols_metadata_df.columns.tolist()\ncolumn_names","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:59:11.619488Z","iopub.execute_input":"2023-11-28T04:59:11.619906Z","iopub.status.idle":"2023-11-28T04:59:11.627636Z","shell.execute_reply.started":"2023-11-28T04:59:11.619862Z","shell.execute_reply":"2023-11-28T04:59:11.626384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nasdaq100_meta","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:59:12.015360Z","iopub.execute_input":"2023-11-28T04:59:12.015756Z","iopub.status.idle":"2023-11-28T04:59:12.031122Z","shell.execute_reply.started":"2023-11-28T04:59:12.015728Z","shell.execute_reply":"2023-11-28T04:59:12.029559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters\n","metadata":{}},{"cell_type":"code","source":"LEARNING_RATE = 1e-2\nBATCH_SIZE = 4\nTEST_BATCH_SIZE = 1\nNUM_EPOCHS = 5\nDEBUG = False\n\ndataset_name = \"stock_closing_price\"\n\ntrain_mse_loss_avg_values = []\ntrain_rmse_loss_avg_values = []\ntrain_f1_avg_values = []\n\nvalid_mse_loss_avg_values = []\nvalid_rmse_loss_avg_values = []\nvalid_f1_avg_values = []","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:59:14.978808Z","iopub.execute_input":"2023-11-28T04:59:14.979576Z","iopub.status.idle":"2023-11-28T04:59:14.986324Z","shell.execute_reply.started":"2023-11-28T04:59:14.979531Z","shell.execute_reply":"2023-11-28T04:59:14.985204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Function","metadata":{}},{"cell_type":"code","source":"def mkdir_prep_dir(dirpath):\n    \"\"\"make preprocess directory if doesn't exist\"\"\"\n    prep_dir = dirpath\n    if not os.path.exists(prep_dir):\n        os.makedirs(prep_dir)\n    return prep_dir","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:59:16.486418Z","iopub.execute_input":"2023-11-28T04:59:16.487164Z","iopub.status.idle":"2023-11-28T04:59:16.493292Z","shell.execute_reply.started":"2023-11-28T04:59:16.487120Z","shell.execute_reply":"2023-11-28T04:59:16.492471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PyTorch Functions & Classes","metadata":{}},{"cell_type":"code","source":"def save_close_price_mavg(stock_close_prices, mavg_n_days_close_prices, dst_folder=\"plots/data_prep/csv\", filename=\"mavg_vs_close_prices.csv\"):\n    mkdir_prep_dir(f\"{dst_folder}\")\n    dst_filepath = f\"{dst_folder}/{filename}\"\n\n    # final close price pred for each 100th date after predicting on 100 days of final closing prices\n    stock_close_price_res_df = pd.DataFrame({\"Close\": stock_close_prices, \"MAVG\": mavg_n_days_close_prices})\n    stock_close_price_res_df.to_csv(dst_filepath, index=False)\n    return stock_close_price_res_df\n\ndef save_checkpoint(state, filename=\"best_lstm.pth.tar\"):\n    print(\"=> Saving Checkpoint\")\n    torch.save(state, filename)\n\ndef rmse(y_true, y_pred):\n    return torch.sqrt(torch.mean((y_true - y_pred) ** 2))\n\n# Based on perplexity.ai solution to torchmetrics.functional f1 lib incompatibility\n# https://www.perplexity.ai/search/Stock-Recommender-System-BY7xxwMfT9Ohxqz5klk1BA?s=c\ndef f1_score(y_pred, y_true):\n    epsilon = 1e-7\n    tp = (y_true * y_pred).sum().to(torch.float32)\n    fp = ( (1 - y_true) * y_pred ).sum().to(torch.float32)\n    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n    precision = tp / (tp + fp + epsilon)\n    recall = tp / (tp + fn + epsilon)\n    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n    return f1\n\nclass NasdaqTimeSeriesDataset(torch.utils.data.Dataset):\n    def __init__(self, stock_close_price_fets, stock_close_price_labels):\n        self.stock_close_price_fets_ = stock_close_price_fets\n        self.stock_close_price_labels_ = stock_close_price_labels\n        self.count_ = 0\n\n    def __len__(self):\n        return len(self.stock_close_price_fets_)\n\n    def __getitem__(self, idx):\n        if DEBUG and self.count_ == 0:\n            print(f\"NasdaqDataset __getitem__ idx = {idx}\")\n        # each array holds 100 days of closing prices\n        closing_price_features = self.stock_close_price_fets_[idx]\n        closing_price_features_tensor = torch.tensor(closing_price_features).float()\n\n        if DEBUG and self.count_ == 0:\n            print(f\"closing_price_features_tensor.shape = {closing_price_features_tensor.shape}\")\n        \n        closing_price_labels = self.stock_close_price_labels_[idx]\n        closing_price_labels_tensor = torch.tensor(closing_price_labels).float()\n\n        if DEBUG and self.count_ == 0:\n            print(f\"closing_price_labels_tensor.shape = {closing_price_labels_tensor.shape}\")\n\n        self.count_ += 1\n        \n        return closing_price_features_tensor, closing_price_labels_tensor\n    \n# Ideally, I am thinking of parts of the preprocessing pipeline code, I can refactor toward nifi later\n# Need a better telemetry dashboard to interact with NiFi: Unity, H2O Wave, PySide6 QT\nclass DataPrepPipeline:\n    def __init__(self):\n        pass\n\n    # Based on Greg Hogg's Stock Pred Video: https://youtu.be/q_HS4s1L8UI?si=CHWvbvfoPExQcALa\n    def prepare_df_for_lstm(self, stock_df, n_steps):\n        \"\"\"\n        # 10 days stock price\n        # it'll take the first 10 days, to then predict 11th day stock price\n        # 10, 11, 12, 15, 16, 17, 18, 19, 20, 17, \"18\"\n        # for the 12th day prediction, it'll take the 11th back to the 1st day\n        # .., 11, 12, 15, 16, 17, 18, 19, 20, 17, 18, \"19\"\n        \"\"\"\n        stock_df = dc(stock_df)\n        stock_df[\"Date\"] = pd.to_datetime(stock_df[\"Date\"])\n\n        stock_df.set_index(\"Date\", inplace=True)\n\n        for i in range(1, n_steps+1):\n            stock_df[f\"Close(t-{i})\"] = stock_df[\"Close\"].shift(i)\n\n        stock_df.dropna(inplace=True)\n\n        return stock_df\n\n    def split_features_labels(self, stock_features_labels_pd):\n        x_features = stock_features_labels_pd[:, 1:]\n        y_labels = stock_features_labels_pd[:, 0]\n\n        if DEBUG:\n            print(f\"x_features shape = {x_features.shape}; y_labels shape = {y_labels.shape}\")\n        return x_features, y_labels\n    \nclass PyTorchPipeline:\n    def __init__(self):\n        pass\n\n    def train_model(self, train_loader, lstm_model, optimizer, loss_criterion):\n        train_loop = tqdm(train_loader)\n\n        train_mse_loss_values = []\n        train_rmse_loss_values = []\n        train_f1_metric_values = []\n\n        # closing_price_2d_tensor\n        for batch_idx, (closing_price_2d_tensor, final_closing_price_tensor_gt) in enumerate(train_loop):\n            # final_closing_price_tensor_gt = final_closing_price_tensor.unsqueeze(1)\n\n            optimizer.zero_grad()\n            closing_price_2d_tensor = closing_price_2d_tensor.to(device)\n            final_closing_price_tensor_gt = final_closing_price_tensor_gt.to(device)\n            pred_final_closing_price = lstm_model(closing_price_2d_tensor)\n            mse_loss = loss_criterion(pred_final_closing_price, final_closing_price_tensor_gt)\n            rmse_loss = rmse(pred_final_closing_price, final_closing_price_tensor_gt)\n\n            f1_metric = f1_score(pred_final_closing_price, final_closing_price_tensor_gt)\n            \n            mse_loss.backward()\n            optimizer.step()\n\n            train_mse_loss_values.append(mse_loss.item())\n            train_rmse_loss_values.append(rmse_loss.item())\n            train_f1_metric_values.append(f1_metric.item())\n\n            train_loop.set_postfix(train_mse_loss=mse_loss.item(), train_rmse_loss=rmse_loss.item(), train_f1=f1_metric.item())\n\n        train_mse_loss_avg_values.append( sum(train_mse_loss_values)/len(train_loader) )\n        train_rmse_loss_avg_values.append( sum(train_rmse_loss_values)/len(train_loader) )\n        train_f1_avg_values.append( sum(train_f1_metric_values)/len(train_loader) )\n\n    def validate_model(self, val_loader, lstm_model, loss_criterion):\n        val_loop = tqdm(val_loader)\n\n        val_mse_loss_values = []\n        val_rmse_loss_values = []\n        val_f1_metric_values = []\n\n        for batch_idx, (closing_price_2d_tensor, final_closing_price_tensor_gt) in enumerate(val_loop):\n            # final_closing_price_tensor_gt = final_closing_price_tensor.unsqueeze(1)\n            closing_price_2d_tensor = closing_price_2d_tensor.to(device)\n            final_closing_price_tensor_gt = final_closing_price_tensor_gt.to(device)\n            \n            pred_final_closing_price = lstm_model(closing_price_2d_tensor)\n            mse_loss = loss_criterion(pred_final_closing_price, final_closing_price_tensor_gt)\n            rmse_loss = rmse(pred_final_closing_price, final_closing_price_tensor_gt)\n\n            f1_metric = f1_score(pred_final_closing_price, final_closing_price_tensor_gt)\n            \n            val_mse_loss_values.append(mse_loss.item())\n            val_rmse_loss_values.append(rmse_loss.item())\n            val_f1_metric_values.append(f1_metric.item())\n\n            val_loop.set_postfix(val_mse_loss=mse_loss.item(), val_rmse_loss=rmse_loss.item(), val_f1=f1_metric.item())\n\n        valid_mse_loss_avg_values.append( sum(val_mse_loss_values)/len(val_loader) )\n        valid_rmse_loss_avg_values.append( sum(val_rmse_loss_values)/len(val_loader) )\n        valid_f1_avg_values.append( sum(val_f1_metric_values)/len(val_loader) )\n\n        \n    def train_over_epochs(self, train_loader, test_loader, lstm_model, optimizer, loss_criterion, dst_folder=\"lstm_model\"):\n        step = 100\n        for epoch in range(NUM_EPOCHS):\n            self.train_model(train_loader, lstm_model, optimizer, loss_criterion)\n\n            checkpoint = {\n                \"state_dict\": lstm_model.state_dict(),\n                \"optimizer\": optimizer.state_dict(),\n            }\n\n            mkdir_prep_dir(f\"{dataset_name}/{dst_folder}\")\n            filename = f\"{dataset_name}/{dst_folder}/lstm_model_{step}.pth.tar\"\n            save_checkpoint(checkpoint, filename=filename)\n\n            self.validate_model(test_loader, lstm_model, loss_criterion)\n\n            step += 100\n\n    def deploy_model(self, test_loader, lstm_model):\n        test_loop = tqdm(test_loader)\n        final_closing_price_predictions = []\n        final_closing_price_gts = []\n        count = 0\n\n        for batch_idx, (closing_price_2d_tensor, final_closing_price_tensor) in enumerate(test_loop):\n            closing_price_2d_tensor = closing_price_2d_tensor.to(device)\n            final_closing_price_pred = lstm_model(closing_price_2d_tensor)\n            # if DEBUG and count == 0:\n#             print(f\"final_closing_price_pred.item() = {final_closing_price_pred.item()}\")\n            final_closing_price_predictions.append(final_closing_price_pred.item())\n            final_closing_price_gts.append(final_closing_price_tensor.item())\n            count += 1\n        return final_closing_price_gts, final_closing_price_predictions\n    \n    def test_dataloader_sample(self, data_loader):\n        for batch_idx, (closing_price_features_tensor, closing_price_labels_tensor) in enumerate(data_loader):\n            closing_price_features_tensor = closing_price_features_tensor.to(device)\n            closing_price_labels_tensor = closing_price_labels_tensor.to(device)\n            print(f\"close price features shape: {closing_price_features_tensor.shape}; close price labels shape: {closing_price_labels_tensor.shape}\")\n            break\n    \nclass PyTorchUtility:\n    def __init__(self):\n        pass\n\n    # TODO (JG): Add arguments for naming the two labels\n    def plot_loss_curves(self, train_loss_values, test_loss_values, plot_title, plot_filename):\n        f, ax = plt.subplots()\n        ax.set_title(plot_title)\n        ax.plot(train_loss_values, color=\"blue\", label=\"Train RMSE\")\n        ax.plot(test_loss_values, color=\"gold\", label=\"Valid RMSE\")\n        ax.grid(True)\n        ax.set_xlabel(\"Epochs\")\n        ax.set_ylabel(\"RMSE Loss\")\n        plt.legend()\n        plt.savefig(plot_filename)\n        plt.show()\n\n    \n    def plot_gt_pred_curves(self, final_closing_price_gts, final_closing_price_predictions, plot_title, plot_filename):\n        f, ax = plt.subplots()\n        ax.set_title(plot_title)\n        ax.plot(final_closing_price_predictions, color=\"blue\", label=\"Pred Price\")\n        ax.plot(final_closing_price_gts, color=\"gold\", label=\"GT Price\")\n        ax.grid(True)\n        ax.set_xlabel(\"Time\")\n        ax.set_ylabel(\"Price\")\n        plt.legend()\n        plt.savefig(plot_filename)\n        plt.show()\n\n    def plot_f1_curves(self, train_f1_values, test_f1_values, plot_title, plot_filepath):\n        # mkdir_prep_dir(plot_filepath)\n        f, ax = plt.subplots()\n        ax.set_title(plot_title)\n        ax.plot(train_f1_values, color=\"blue\", label=\"Train F1\")\n        ax.plot(test_f1_values, color=\"gold\", label=\"Valid F1\")\n        ax.grid(True)\n        ax.set_xlabel(\"Epochs\")\n        ax.set_ylabel(\"F1 Score\")\n        plt.legend()\n        plt.savefig(plot_filepath)\n        plt.show()\n        \n\n    # TODO (JG): Need to check num_days preds for closing price. Is it suppose to be over\n    def save_lstm_predictions(self, final_closing_price_dates, final_closing_price_predictions, scale, num_days, dst_folder=\"lstm_model\", filename=\"pytorch_final_closing_price_preds.csv\"):\n        mkdir_prep_dir(f\"{dataset_name}/{dst_folder}\")\n        dst_filepath = f\"{dataset_name}/{dst_folder}/{filename}\"\n\n        # final close price pred for each 100th date after predicting on 100 days of final closing prices\n#         print(final_closing_price_dates[:5], final_closing_price_predictions[:5])\n        stock_close_price_res_df = pd.DataFrame({f\"{num_days}th Date\": final_closing_price_dates, \"Close\": final_closing_price_predictions})\n        stock_close_price_res_df.to_csv(dst_filepath, index=False)\n        return stock_close_price_res_df\n    \n# BEST Results so far with regards to MSE, RMSE, etc\nclass StockClosePriceLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size=10, num_stacked_layers=1, dropout_ratio=0.2):\n        super(StockClosePriceLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_stacked_layers = num_stacked_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers, batch_first=True, \n                            dropout=dropout_ratio, bias=True)\n\n        self.fc = nn.Linear(hidden_size, 1)\n        # self.dropout1 = nn.Dropout(0.2)\n        # self.relu = nn.ReLU()\n\n        self.count_ = 0\n\n    def forward(self, closing_price_2d_features):\n        # closing price 2D features: row by col, each row has 100 samples since 100 days of closing prices, \n        # trying to pred final closing price\n        if DEBUG and self.count_ == 0:\n            print(f\"type(closing_price_2d_features) = {type(closing_price_2d_features)}\")\n        batch_size = closing_price_2d_features.size(0)\n        if DEBUG and self.count_ == 0:\n            print(f\"closing_price_2d_features batch_size = {batch_size}\")\n        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size).to(device)\n\n        \n        lstm_features, lstm_hidden = self.lstm(closing_price_2d_features, (h0, c0))\n        # decode hidden state of the last time step\n        lstm_features = lstm_features[:, -1, :]\n        if DEBUG and self.count_ == 0:\n            print(f\"type(lstm_features) = {type(lstm_features)}\")\n        stock_price_pred = self.fc(lstm_features)\n\n        self.count_ += 1\n        return stock_price_pred\n    \n# NOTE: Not so great results when trying multiple LSTM layers with varying hidden features\nclass StockPriceLSTMV2(nn.Module):\n    def __init__(self, input_size, hidden_sizes=[10, 20, 30, 40, 50], num_stacked_layers=5, dropout_ratio=0.3, num_classes=1):\n        super(StockPriceLSTMV2, self).__init__()\n        self.hidden_sizes = hidden_sizes\n        self.num_stacked_layers = num_stacked_layers\n\n        self.lstm_list = nn.ModuleList([\n            nn.LSTM(input_size, self.hidden_sizes[0], \n                    batch_first=True, dropout=dropout_ratio, bias=True)\n        ])\n\n        for i in range(1, self.num_stacked_layers):\n            if i < self.num_stacked_layers-1:\n                print(f\"Not on last LSTM layer {i}, adding dropout = {dropout_ratio}\")\n                self.lstm_list.append( nn.LSTM(self.hidden_sizes[i-1], self.hidden_sizes[i], \n                                              batch_first=True, dropout=dropout_ratio, bias=True) )\n            else:\n                print(f\"On last LSTM layer {i}, ignoring dropout = {dropout_ratio}\")\n                self.lstm_list.append( nn.LSTM(self.hidden_sizes[i-1], self.hidden_sizes[i], \n                                              batch_first=True, bias=True) )\n        \n        # self.lstm_list.extend([\n        #     nn.LSTM(self.hidden_sizes[i-1], self.hidden_sizes[i], batch_first=True, dropout=dropout_ratio, bias=True)\n        #         for i in range(1, self.num_stacked_layers)\n        # ])\n\n        self.fc = nn.Linear(self.hidden_sizes[-1], num_classes)\n        self.count_ = 0\n\n    def forward(self, stock_price_features):\n        if DEBUG and self.count_ == 0:\n            print(f\"type(stock_price_features) = {type(stock_price_features)}\")\n        batch_size = stock_price_features.size(0)\n        if DEBUG and self.count_ == 0:\n            print(f\"stock_price_features batch_size = {batch_size}\")\n\n        hiddens = [torch.zeros(batch_size, hidden_size).to(stock_price_features.device)\n                       for hidden_size in self.hidden_sizes]\n\n        for i in range(self.num_stacked_layers):\n            stock_price_features, (hiddens[i], _) = self.lstm_list[i](stock_price_features, (\n                hiddens[i].unsqueeze(0), hiddens[i].unsqueeze(0)\n            ) )\n        \n        stock_price_features = stock_price_features[:, -1, :]\n        if DEBUG and self.count_ == 0:\n            print(f\"type(stock_price_features) = {type(stock_price_features)}\")\n        stock_price_pred = self.fc(stock_price_features)\n\n        self.count_ += 1\n        return stock_price_pred\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:59:25.531193Z","iopub.execute_input":"2023-11-28T04:59:25.531907Z","iopub.status.idle":"2023-11-28T04:59:25.596226Z","shell.execute_reply.started":"2023-11-28T04:59:25.531855Z","shell.execute_reply":"2023-11-28T04:59:25.595292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TensorFlow Keras Functions & Classes","metadata":{}},{"cell_type":"code","source":"class Time2Vector(Layer):\n    def __init__(self, sequence, **kwargs):\n        super(Time2Vector, self).__init__()\n        self.sequence = sequence\n\n    def build(self, input_shape):\n        self.weights_linear = self.add_weight(name='weight_linear',\n                                    shape=(int(self.sequence),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n        self.bias_linear = self.add_weight(name='bias_linear',\n                                    shape=(int(self.sequence),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n        self.weights_periodic = self.add_weight(name='weight_periodic',\n                                    shape=(int(self.sequence),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n        self.bias_periodic = self.add_weight(name='bias_periodic',\n                                    shape=(int(self.sequence),),\n                                    initializer='uniform',\n                                    trainable=True)\n\n    def call(self, x):\n        x = tf.math.reduce_mean(x[:,:,:4], axis=-1) \n        time_linear = self.weights_linear * x + self.bias_linear\n        time_linear = tf.expand_dims(time_linear, axis=-1) \n\n        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n        time_periodic = tf.expand_dims(time_periodic, axis=-1) \n        return tf.concat([time_linear, time_periodic], axis=-1) \n\n    def get_config(self): \n        config = super().get_config().copy()\n        config.update({'sequence': self.sequence})\n        return config\n\nclass SingleAttention(Layer):\n    def __init__(self, d_k, d_v):\n        super(SingleAttention, self).__init__()\n        self.d_k = d_k\n        self.d_v = d_v\n\n    def build(self, input_shape):\n        self.query = Dense(self.d_k, \n                           input_shape=input_shape, \n                           kernel_initializer='glorot_uniform', \n                           bias_initializer='glorot_uniform')\n\n        self.key = Dense(self.d_k, \n                         input_shape=input_shape, \n                         kernel_initializer='glorot_uniform', \n                         bias_initializer='glorot_uniform')\n\n        self.value = Dense(self.d_v, \n                           input_shape=input_shape, \n                           kernel_initializer='glorot_uniform', \n                           bias_initializer='glorot_uniform')\n\n    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n        q = self.query(inputs[0])\n        k = self.key(inputs[1])\n\n        attn_weights = tf.matmul(q, k, transpose_b=True)\n        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n        v = self.value(inputs[2])\n        attn_out = tf.matmul(attn_weights, v)\n        return attn_out    \n\nclass MultiAttention(Layer):\n    def __init__(self, d_k, d_v, n_heads):\n        super(MultiAttention, self).__init__()\n        self.d_k = d_k\n        self.d_v = d_v\n        self.n_heads = n_heads\n        self.attn_heads = list()\n\n    def build(self, input_shape):\n        for n in range(self.n_heads):\n            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n\n        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n        self.linear = Dense(input_shape[0][-1], \n                            input_shape=input_shape, \n                            kernel_initializer='glorot_uniform', \n                            bias_initializer='glorot_uniform')\n\n    def call(self, inputs):\n        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n        concat_attn = tf.concat(attn, axis=-1)\n        multi_linear = self.linear(concat_attn)\n        return multi_linear   \n\nclass TransformerEncoder(Layer):\n    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n        super(TransformerEncoder, self).__init__()\n        self.d_k = d_k\n        self.d_v = d_v\n        self.n_heads = n_heads\n        self.ff_dim = ff_dim\n        self.attn_heads = list()\n        self.dropout_rate = dropout\n\n    def build(self, input_shape):\n        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n        self.attn_dropout = Dropout(self.dropout_rate)\n        self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n\n        self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n        self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n        self.ff_dropout = Dropout(self.dropout_rate)\n        self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n\n    def call(self, inputs): \n        attn_layer = self.attn_multi(inputs)\n        attn_layer = self.attn_dropout(attn_layer)\n        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n\n        ff_layer = self.ff_conv1D_1(attn_layer)\n        ff_layer = self.ff_conv1D_2(ff_layer)\n        ff_layer = self.ff_dropout(ff_layer)\n        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n        return ff_layer \n\n    def get_config(self): \n        config = super().get_config().copy()\n        config.update({'d_k': self.d_k,\n                       'd_v': self.d_v,\n                       'n_heads': self.n_heads,\n                       'ff_dim': self.ff_dim,\n                       'attn_heads': self.attn_heads,\n                       'dropout_rate': self.dropout_rate})\n        return config  \n\ndef create_model():\n    time_embedding = Time2Vector(sequence)\n    attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n    attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n\n    in_seq = Input(shape=(sequence, 6))\n    x = time_embedding(in_seq)\n    x = Concatenate(axis=-1)([in_seq, x])\n    x = attn_layer1((x, x, x))\n    x = attn_layer2((x, x, x))\n\n    x = GlobalAveragePooling1D(data_format='channels_first')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    out = Dense(1, activation='linear')(x)\n\n    model = Model(inputs=in_seq, outputs=out)\n    model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:59:28.224175Z","iopub.execute_input":"2023-11-28T04:59:28.224534Z","iopub.status.idle":"2023-11-28T04:59:28.258376Z","shell.execute_reply.started":"2023-11-28T04:59:28.224506Z","shell.execute_reply":"2023-11-28T04:59:28.257279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train & Test PyTorch LSTM Model \n\non stock closing price predictions","metadata":{}},{"cell_type":"code","source":"one_hundred_num_days = 100\ntwo_hundred_num_days = 200\napple_mavg_100_days = stock_dfs['A'].Close.rolling(one_hundred_num_days).mean()\napple_mavg_200_days = stock_dfs['A'].Close.rolling(two_hundred_num_days).mean()\n\ntrain_ratio = 0.60\nvalid_ratio = 0.20 + train_ratio # 0.80\ntest_ratio = 0.20\n\nlookback = 100\n\nscaler = MinMaxScaler(feature_range=(0,1))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T05:00:31.336062Z","iopub.execute_input":"2023-11-28T05:00:31.336463Z","iopub.status.idle":"2023-11-28T05:00:31.346848Z","shell.execute_reply.started":"2023-11-28T05:00:31.336428Z","shell.execute_reply":"2023-11-28T05:00:31.345678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tickers = ['AAPL', 'ACH', 'ACGL', 'ACER', 'ABT']","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:59:32.276035Z","iopub.execute_input":"2023-11-28T04:59:32.276584Z","iopub.status.idle":"2023-11-28T04:59:32.280914Z","shell.execute_reply.started":"2023-11-28T04:59:32.276557Z","shell.execute_reply":"2023-11-28T04:59:32.279899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First get moving average 100 vs closing price predictions\n# mkdir_prep_dir(\"/kaggle/working/stock_closing_price/data_explore/plots\")\nmkdir_prep_dir(\"/kaggle/working/stock_closing_price/data_explore/mavg100_close_price\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T04:59:51.996641Z","iopub.execute_input":"2023-11-28T04:59:51.997126Z","iopub.status.idle":"2023-11-28T04:59:52.004582Z","shell.execute_reply.started":"2023-11-28T04:59:51.997088Z","shell.execute_reply":"2023-11-28T04:59:52.003456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save At least 5 company tickers in data explore folder\nfor ticker_stock in tickers:\n    mavg_100_days = stock_dfs[ticker_stock].Close.rolling(lookback).mean()\n    stock_close_prices = stock_dfs[ticker_stock].Close\n    save_close_price_mavg(stock_close_prices, mavg_100_days,\n                          dst_folder=f\"/kaggle/working/stock_closing_price/data_explore/mavg100_close_price\", \n                          filename=f\"{ticker_stock}_mavg100_vs_close_prices.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-28T05:00:38.910583Z","iopub.execute_input":"2023-11-28T05:00:38.910986Z","iopub.status.idle":"2023-11-28T05:00:39.122485Z","shell.execute_reply.started":"2023-11-28T05:00:38.910957Z","shell.execute_reply":"2023-11-28T05:00:39.121166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nifi_pipeline = DataPrepPipeline()\npytorch_pipeline = PyTorchPipeline()\npytorch_utility = PyTorchUtility()\n\nfor ticker_stock in tickers:\n    print(f\"Training PyTorch LSTM on {ticker_stock}\")\n    ticker_pd = stock_dfs[ticker_stock]\n    ticker_pd = ticker_pd[[\"Date\", \"Close\"]]\n    # Array slicing on 1st 100 Days Stock Data\n    ticker_stock_fets_labels_df = nifi_pipeline.prepare_df_for_lstm(ticker_pd, n_steps=lookback)\n    ticker_pd_scale = scaler.fit_transform(ticker_stock_fets_labels_df)\n    x_stock_fets_list, y_stock_labels_list = nifi_pipeline.split_features_labels(ticker_pd_scale)\n    # 60% for training: 0 to 60%\n    train_split_index = int(len(x_stock_fets_list) * train_ratio)\n    valid_split_index = int(len(x_stock_fets_list)*valid_ratio)\n    test_split_index = len(x_stock_fets_list)\n    X_ticker_stock_train = x_stock_fets_list[0: train_split_index]\n    \n    # 20% for validation: 60% to 80%\n    X_ticker_stock_valid = x_stock_fets_list[train_split_index: valid_split_index]\n    \n    X_ticker_stock_test = x_stock_fets_list[valid_split_index: test_split_index]\n    \n    y_ticker_stock_train = y_stock_labels_list[0: train_split_index]\n\n    y_ticker_stock_valid = y_stock_labels_list[train_split_index: valid_split_index]\n\n    y_ticker_stock_test = y_stock_labels_list[valid_split_index: test_split_index]\n    \n    Date_ticker_stock_train = ticker_pd.index[0: train_split_index]\n    Date_ticker_stock_valid = ticker_pd.index[train_split_index: valid_split_index]\n    Date_ticker_stock_test = ticker_pd.index[valid_split_index: test_split_index]\n    \n    # Add extra dimension for LSTM prep\n    X_ticker_stock_train = X_ticker_stock_train.reshape( (-1, lookback, 1) )\n    X_ticker_stock_valid = X_ticker_stock_valid.reshape( (-1, lookback, 1) )\n    X_ticker_stock_test = X_ticker_stock_test.reshape( (-1, lookback, 1) )\n\n    y_ticker_stock_train = y_ticker_stock_train.reshape( (-1, 1) )\n    y_ticker_stock_valid = y_ticker_stock_valid.reshape( (-1, 1) )\n    y_ticker_stock_test = y_ticker_stock_test.reshape( (-1, 1) )\n\n    stock_train_dataset = NasdaqTimeSeriesDataset(X_ticker_stock_train, y_ticker_stock_train)\n\n    stock_valid_dataset = NasdaqTimeSeriesDataset(X_ticker_stock_valid, y_ticker_stock_valid)\n    stock_test_dataset = NasdaqTimeSeriesDataset(X_ticker_stock_test, y_ticker_stock_test)\n    \n    # Create PyTorch Nasdaq DataLoader (Train, Test)\n    stock_train_loader = DataLoader(stock_train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n    stock_valid_loader = DataLoader(stock_valid_dataset, batch_size = TEST_BATCH_SIZE, shuffle=False)\n    stock_test_loader = DataLoader(stock_test_dataset, batch_size = TEST_BATCH_SIZE, shuffle=False)\n    \n    # Choose model\n    lstm_model = StockClosePriceLSTM(input_size=1, hidden_size=10, num_stacked_layers=1, dropout_ratio=0.3)\n    lstm_model.to(device)\n    mse_criterion = nn.MSELoss()\n    lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)\n\n    pytorch_pipeline.train_over_epochs(\n        stock_train_loader, stock_valid_loader,\n        lstm_model,\n        lstm_optimizer,\n        mse_criterion,\n        dst_folder=\"lstm_stock_close\"\n    )\n    \n    # Get predictions\n    final_closing_price_gts, final_closing_price_predictions = pytorch_pipeline.deploy_model(stock_test_loader, lstm_model)\n\n    unscale = 1/scaler.scale_\n    final_closing_price_unscaled_preds = final_closing_price_predictions*np.array([unscale[0]])\n    final_closing_price_unscaled_gts = final_closing_price_gts*np.array([unscale[0]])\n\n    pytorch_utility.save_lstm_predictions(Date_ticker_stock_test, final_closing_price_unscaled_preds, np.array([unscale[0]]),\n                                       num_days=100, dst_folder=\"predictions/\", \n                                       filename=f\"{ticker_stock}_pytorch_final_closing_price_preds.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-26T00:21:51.271969Z","iopub.execute_input":"2023-11-26T00:21:51.272368Z","iopub.status.idle":"2023-11-26T00:24:45.756985Z","shell.execute_reply.started":"2023-11-26T00:21:51.272333Z","shell.execute_reply":"2023-11-26T00:24:45.756082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train & Test TensorFlow Keras Transformer Model\n\non stock closing price predictions","metadata":{}},{"cell_type":"code","source":"sequence = 128\nbatch_size = 128\nd_k = 64\nd_v = 64\nn_heads = 4\nff_dim = 64","metadata":{"execution":{"iopub.status.busy":"2023-11-26T00:25:14.868839Z","iopub.execute_input":"2023-11-26T00:25:14.869568Z","iopub.status.idle":"2023-11-26T00:25:14.874148Z","shell.execute_reply.started":"2023-11-26T00:25:14.869534Z","shell.execute_reply":"2023-11-26T00:25:14.873235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Transformers Utils\n\ndef prepare_data(data, sequence):\n    X, y = [], []\n    for i in range(sequence, len(data)):\n        X.append(data[i - sequence:i])\n        y.append(data[:, 3][i]) \n    return np.array(X), np.array(y)\n\ndef save_tfms_predictions(stock_test_df, final_closing_price_predictions, dst_folder=\"lstm_model\", filename=\"pytorch_final_closing_price_preds.csv\"):\n    mkdir_prep_dir(f\"{dataset_name}/{dst_folder}\")\n    dst_filepath = f\"{dataset_name}/{dst_folder}/{filename}\"\n\n    # final close price pred for each 100th date after predicting on 100 days of final closing prices\n    stock_test_df['Close'] = final_closing_price_predictions\n    stock_test_df.to_csv(dst_filepath, index=False)\n    print(\"\\nExported to csv successfully...\")\n    return stock_test_df\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T00:25:16.495941Z","iopub.execute_input":"2023-11-26T00:25:16.496671Z","iopub.status.idle":"2023-11-26T00:25:16.503658Z","shell.execute_reply.started":"2023-11-26T00:25:16.496634Z","shell.execute_reply":"2023-11-26T00:25:16.502682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ticker_stock in tickers:\n    # Loading individual ticker\n    \n    ticker_pd = stock_dfs[ticker_stock]\n    ticker_pd.rename(columns={'Adj Close': 'Adj_Close'}, inplace=True)\n    \n    ticker_pd.set_index('Date', inplace=True)\n    ticker_pd.sort_index(inplace=True)\n    \n    cols = ['Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']\n    ticker_pd[cols] = ticker_pd[cols].rolling(100).mean()\n    \n    ticker_pd.dropna(axis=0, inplace=True)\n    \n    scaler = MinMaxScaler().set_output(transform=\"pandas\")\n    ticker_pd = scaler.fit_transform(ticker_pd)\n    \n    train_df = ticker_pd.iloc[: int(0.8*len(ticker_pd))]\n    val_df = ticker_pd.iloc[int(0.8*len(ticker_pd)): int(0.9*len(ticker_pd))]\n    test_df = ticker_pd.iloc[int(0.9*len(ticker_pd)): ]\n    print(train_df.shape, val_df.shape, test_df.shape) \n    \n    # Preparing model input and labels\n    X_train, y_train = prepare_data(train_df.values, sequence)\n    X_val, y_val = prepare_data(val_df.values, sequence)\n    X_test, y_test = prepare_data(test_df.values , sequence)\n    \n    print('Training set shape: ', X_train.shape, y_train.shape)\n    print('Validation set shape: ', X_val.shape, y_val.shape)\n    print('Testing set shape: ' ,X_test.shape, y_test.shape)\n    \n    t_model = create_model()\n    callback = tf.keras.callbacks.ModelCheckpoint('Transformer_timeEmbd.hdf5', monitor = 'val_loss', save_best_only = True, verbose = 1)\n    history = t_model.fit(X_train, y_train, batch_size = batch_size, epochs = 20, callbacks = [callback], validation_data = (X_val, y_val))\n    t_model = tf.keras.models.load_model('Transformer_timeEmbd.hdf5', \n                                         custom_objects={'Time2Vector': Time2Vector, \n                                                         'SingleAttention': SingleAttention, \n                                                         'MultiAttention': MultiAttention,\n                                                         'TransformerEncoder': TransformerEncoder})\n    \n    test_predictions = t_model.predict(X_test)\n    close_idx = list(scaler.feature_names_in_).index(\"Close\")\n    min_close, max_close = scaler.data_max_[close_idx], scaler.data_min_[close_idx]\n    test_predictions = test_predictions * (max_close - min_close) + min_close\n    \n    test_df.reset_index(inplace=True)\n    save_tfms_predictions(test_df[['Date']].iloc[sequence:], test_predictions, \n                          dst_folder=\"predictions/\", \n                          filename=f\"{ticker_stock}_transformers_final_closing_price_preds.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-26T00:25:18.361599Z","iopub.execute_input":"2023-11-26T00:25:18.362453Z","iopub.status.idle":"2023-11-26T00:57:43.094729Z","shell.execute_reply.started":"2023-11-26T00:25:18.362417Z","shell.execute_reply":"2023-11-26T00:57:43.093694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stock Technical Indicator Helper Functions","metadata":{}},{"cell_type":"code","source":"def MACDecision(stock_price_pred_df, model_strategy_type_used):\n    stock_price_pred_df[f\"MACD_diff_{model_strategy_type_used}\"] = ta.trend.macd_diff(stock_price_pred_df.Close)\n    # stock_price_pred_df.loc[(stock_price_pred_df.MACD_diff > 0) & \n    #                         (stock_price_pred_df.MACD_diff.shift(1) < 0), \"Decision MACD\"] = \"Buy\"\n    stock_price_pred_df[f\"Decision MACD_{model_strategy_type_used}\"] = np.where((stock_price_pred_df[f\"MACD_diff_{model_strategy_type_used}\"] > 0) & \n                            (stock_price_pred_df[f\"MACD_diff_{model_strategy_type_used}\"].shift(1) < 0), True, False)\n    \ndef GoldenCrossDecision(stock_price_pred_df, model_strategy_type_used):\n    # if short term moving avg > long term moving avg for the day before, then buy, else sell\n    stock_price_pred_df[f\"SMA20_{model_strategy_type_used}\"] = ta.trend.sma_indicator(stock_price_pred_df.Close, window = 20)\n    stock_price_pred_df[f\"SMA50_{model_strategy_type_used}\"] = ta.trend.sma_indicator(stock_price_pred_df.Close, window = 50)\n    stock_price_pred_df[f\"Signal_{model_strategy_type_used}\"] = np.where(stock_price_pred_df[f\"SMA20_{model_strategy_type_used}\"] > stock_price_pred_df[f\"SMA50_{model_strategy_type_used}\"],\n                                             True, False)\n    stock_price_pred_df[f\"Decision GC_{model_strategy_type_used}\"] = stock_price_pred_df[f\"Signal_{model_strategy_type_used}\"].diff()\n    \ndef RSI_SMA_Decision(stock_price_pred_df, model_strategy_type_used):\n    # NOTE: our PyTorch model was trained on 100 days to predict final closing price\n    stock_price_pred_df[f\"RSI_{model_strategy_type_used}\"] = ta.momentum.rsi(stock_price_pred_df.Close, window=10)\n    # This probably would be simple moving average based on 200 days of predicted closing stock prices\n    stock_price_pred_df[f\"SMA200_{model_strategy_type_used}\"] = ta.trend.sma_indicator(stock_price_pred_df.Close, window = 200)\n    # stock_price_pred_df.loc[(stock_price_pred_df.Close > stock_price_pred_df.SMA200) &\n    #                         (stock_price_pred_df.RSI < 30), \"Decision RSI/SMA\"] = \"Buy\"\n    stock_price_pred_df[f\"Decision RSI/SMA_{model_strategy_type_used}\"] = np.where((stock_price_pred_df.Close > stock_price_pred_df[f\"SMA200_{model_strategy_type_used}\"]) &\n                            (stock_price_pred_df[f\"RSI_{model_strategy_type_used}\"] < 30), True, False)\n    \ndef FinancialStatusDecision(stock_price_pred_df, model_strategy_type_used, symbols_metadata_df, company_ticker):\n    financial_status_val = symbols_metadata_df[symbols_metadata_df.Symbol == company_ticker].iloc[0][\"Financial Status\"]\n    always_buy_condition = pd.isna(financial_status_val) or financial_status_val == \"N\"\n    short_term_decision = stock_price_pred_df[f\"Decision GC_{model_strategy_type_used}\"] is True\n    long_term_decision = stock_price_pred_df[f\"Decision RSI/SMA_{model_strategy_type_used}\"] is True\n    conditionaly_buy_condition = (financial_status_val == \"D\" or financial_status_val == \"E\" or financial_status_val == \"H\") and short_term_decision and not long_term_decision\n    stock_price_pred_df[\"Decision Financial Status\"] = np.where(always_buy_condition | conditionaly_buy_condition, True, False)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T01:17:56.560132Z","iopub.execute_input":"2023-11-26T01:17:56.561055Z","iopub.status.idle":"2023-11-26T01:17:56.573838Z","shell.execute_reply.started":"2023-11-26T01:17:56.561024Z","shell.execute_reply":"2023-11-26T01:17:56.572457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stock Predicted Close Price Indicator Recommender System\n\nIngest Stock Close Price Predicted Data and then perform stock closing price recommendation based on technical indicators.","metadata":{}},{"cell_type":"code","source":"stock_price_predict_pds = {}\ndirectory_path = \"/kaggle/working/stock_closing_price/predictions\"\ndirectory_path_recommend_output = \"/kaggle/working/stock_closing_price\"\nfor file in os.listdir(directory_path):\n    if file.endswith('.csv'):\n        stock_price_pred_pd = pd.read_csv(os.path.join(directory_path, file))\n        filename_without_extension = os.path.splitext(file)[0]  # Get filename without extension\n        filename_splitted = filename_without_extension.split(\"_\")\n        print(filename_splitted)\n        company_ticker = filename_splitted[0]\n        model_strategy_type_used = filename_splitted[1]\n        MACDecision(stock_price_pred_pd, model_strategy_type_used)\n        GoldenCrossDecision(stock_price_pred_pd, model_strategy_type_used)\n        RSI_SMA_Decision(stock_price_pred_pd, model_strategy_type_used)\n        FinancialStatusDecision(stock_price_pred_pd, model_strategy_type_used, symbols_metadata_df, company_ticker)\n        if company_ticker in stock_price_predict_pds:\n            # Get columns from df1 that are not in df2\n            columns_to_add = stock_price_pred_pd.columns.difference(stock_price_predict_pds[company_ticker].columns)\n\n            # Add columns from df1 to df2\n            for col in columns_to_add:\n                stock_price_predict_pds[company_ticker][col] = stock_price_pred_pd[col]\n        else:\n            stock_price_predict_pds[company_ticker] = stock_price_pred_pd\n        \n#         if stock_price_pred_pd[\"Decision MACD\"].iloc[-1] == \"True\":\n#             print(f\"Buying Signal MACD for {company_ticker}\")\n#         if stock_price_pred_pd[\"Decision RSI/SMA\"].iloc[-1] == \"True\":\n#             print(f\"Buying Signal RSI/SMA for {company_ticker}\")\n\nfor key, value in stock_price_predict_pds.items():\n    value.to_csv(f\"{directory_path_recommend_output}/{key}_recommend.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T01:18:01.105297Z","iopub.execute_input":"2023-11-26T01:18:01.106057Z","iopub.status.idle":"2023-11-26T01:18:01.365503Z","shell.execute_reply.started":"2023-11-26T01:18:01.106023Z","shell.execute_reply":"2023-11-26T01:18:01.364378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}